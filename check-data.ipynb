{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb280fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configuração dos Diretórios ---\n",
    "PROCESSED_PATH = os.path.join('data', 'processed')\n",
    "OUTPUTS_PATH = os.path.join('data', 'outputs')\n",
    "os.makedirs(OUTPUTS_PATH, exist_ok=True)\n",
    "\n",
    "# --- Caminho do Arquivo Final ---\n",
    "FINAL_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'dados_b3.parquet')\n",
    "\n",
    "print(\"--- CARREGANDO DATAFRAME DO PARQUET ---\")\n",
    "\n",
    "try:\n",
    "    # Carrega o dataset principal, que é a fonte de toda a informação\n",
    "    df_final = pd.read_parquet(FINAL_PARQUET_PATH)\n",
    "    print(f\"\\n[SUCESSO] Dataset principal carregado com {len(df_final):,} registros.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERRO FATAL] O arquivo '{FINAL_PARQUET_PATH}' não foi encontrado. Execute o processamento principal primeiro.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO FATAL] Não foi possível carregar o dataset principal: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PRIMEIRO: Geração de Dicionários\n",
    "# ==============================================================================\n",
    "print(\"\\n\\n--- INICIANDO GERAÇÃO DE DICIONÁRIOS E MAPEAMENTOS ---\")\n",
    "\n",
    "# --- 1. Dicionário Master de Ativos (Security Master) ---\n",
    "print(\"\\n1. Criando o Dicionário Master de Ativos...\")\n",
    "try:\n",
    "    # Usamos o df_final completo para garantir que nenhum ativo seja perdido\n",
    "    df_ativos = df_final[['DATA_PREGAO', 'CODISI', 'CODNEG', 'NOMRES', 'ESPECI']].copy()\n",
    "    df_ativos.dropna(subset=['CODISI'], inplace=True) # Dropar se o ISIN for nulo é seguro\n",
    "\n",
    "    df_base_ativos = (\n",
    "        df_ativos.sort_values('DATA_PREGAO', ascending=False)\n",
    "        .drop_duplicates(subset=['CODISI'], keep='first')\n",
    "        .rename(columns={'CODNEG': 'ULTIMO_TICKER', 'NOMRES': 'ULTIMO_NOME', 'ESPECI': 'ULTIMA_ESPECIFICACAO'})\n",
    "        .drop(columns='DATA_PREGAO')\n",
    "    )\n",
    "\n",
    "    # --- CORREÇÃO AQUI ---\n",
    "    # Modificamos a função lambda para remover valores nulos ANTES de tentar ordenar.\n",
    "    # Usamos pd.notna() para filtrar e str() para garantir que tudo seja texto.\n",
    "    df_variacoes_tickers = (\n",
    "        df_ativos.groupby('CODISI')['CODNEG']\n",
    "        .apply(lambda x: ' | '.join(sorted([str(item) for item in x.unique() if pd.notna(item)])))\n",
    "        .reset_index()\n",
    "        .rename(columns={'CODNEG': 'TICKERS_HISTORICOS'})\n",
    "    )\n",
    "    \n",
    "    # --- E CORREÇÃO AQUI TAMBÉM ---\n",
    "    df_variacoes_nomes = (\n",
    "        df_ativos.groupby('CODISI')['NOMRES']\n",
    "        .apply(lambda x: ' | '.join(sorted([str(item) for item in x.unique() if pd.notna(item)])))\n",
    "        .reset_index()\n",
    "        .rename(columns={'NOMRES': 'NOMES_HISTORICOS'})\n",
    "    )\n",
    "\n",
    "    df_dicionario_ativos = pd.merge(df_base_ativos, df_variacoes_tickers, on='CODISI', how='left')\n",
    "    df_dicionario_ativos = pd.merge(df_dicionario_ativos, df_variacoes_nomes, on='CODISI', how='left')\n",
    "\n",
    "    cols_ordem = [\n",
    "        'CODISI', 'ULTIMO_TICKER', 'ULTIMO_NOME', 'ULTIMA_ESPECIFICACAO', \n",
    "        'TICKERS_HISTORICOS', 'NOMES_HISTORICOS'\n",
    "    ]\n",
    "    df_dicionario_ativos = df_dicionario_ativos[cols_ordem].sort_values('ULTIMO_TICKER').reset_index(drop=True)\n",
    "\n",
    "    output_path_parquet = os.path.join(OUTPUTS_PATH, 'dicionario_ativos.parquet')\n",
    "    output_path_excel = os.path.join(OUTPUTS_PATH, 'dicionario_ativos.xlsx')\n",
    "    \n",
    "    df_dicionario_ativos.to_parquet(output_path_parquet, index=False)\n",
    "    df_dicionario_ativos.to_excel(output_path_excel, index=False)\n",
    "    \n",
    "    print(f\" -> [SUCESSO] Dicionário com {len(df_dicionario_ativos)} ativos únicos gerado.\")\n",
    "    print(f\"    -> Salvo em (Parquet): {output_path_parquet}\")\n",
    "    print(f\"    -> Salvo em (Excel): {output_path_excel}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" -> [ERRO] Falha ao gerar o Dicionário Master de Ativos: {e}\")\n",
    "\n",
    "# --- 2. Dicionários de Códigos da B3 ---\n",
    "print(\"\\n2. Gerando Dicionários de Códigos (CODBDI e TPMERC)...\")\n",
    "try:\n",
    "    # Usei seu mapeamento completo aqui para ser mais robusto\n",
    "    map_codbdi = {\n",
    "        '02': 'LOTE PADRÃO', '05': 'SANCIONADAS', '06': 'CONCORDATÁRIAS', '07': 'RECUPERAÇÃO EXTRAJUDICIAL',\n",
    "        '08': 'RECUPERAÇÃO JUDICIAL', '09': 'REGIME DE ADMINISTRAÇÃO ESPECIAL TEMPORÁRIA',\n",
    "        '10': 'DIREITOS E RECIBOS', '11': 'INTERVENÇÃO', '12': 'FUNDOS IMOBILIÁRIOS', '14': 'CERTIFICADOS DE INVESTIMENTO',\n",
    "        '18': 'OBRIGAÇÕES', '22': 'BÔNUS (PRIVADOS)', '26': 'APÓLICES/BÔNUS/TÍTULOS (PÚBLICOS)',\n",
    "        '32': 'EXERCÍCIO DE OPÇÕES DE COMPRA DE ÍNDICES', '33': 'EXERCÍCIO DE OPÇÕES DE VENDA DE ÍNDICES',\n",
    "        '38': 'EXERCÍCIO DE OPÇÕES DE COMPRA', '42': 'EXERCÍCIO DE OPÇÕES DE VENDA', '46': 'LEILÃO DE AÇÕES EM MORA',\n",
    "        '48': 'LEILÃO DE AÇÕES (ART. 49)', '49': 'LEILÃO DE AÇÕES', '50': 'LEILÃO DE AÇÕES', '51': 'LEILÃO DE AÇÕES',\n",
    "        '52': 'LEILÃO DE AÇÕES', '53': 'LEILÃO DE AÇÕES', '54': 'LEILÃO DE AÇÕES', '56': 'LEILÃO DE AÇÕES',\n",
    "        '58': 'LEILÃO', '60': 'LEILÃO', '61': 'LEILÃO', '62': 'LEILÃO', '66': 'DEBÊNTURES COM DATA DE VENCIMENTO ATÉ 3 ANOS',\n",
    "        '68': 'DEBÊNTURES COM DATA DE VENCIMENTO MAIOR QUE 3 ANOS', '70': 'FUTURO COM RETENÇÃO DE GANHOS',\n",
    "        '71': 'FUTURO COM MOVIMENTAÇÃO DIÁRIA', '74': 'OPÇÕES DE COMPRA DE ÍNDICES', '75': 'OPÇÕES DE VENDA DE ÍNDICES',\n",
    "        '78': 'OPÇÕES DE COMPRA', '82': 'OPÇÕES DE VENDA', '83': 'BOVESPAFIX', '84': 'SOMA FIX', '90': 'TERMO',\n",
    "        '96': 'FRACIONÁRIO', '99': 'TOTAL'\n",
    "    }\n",
    "    df_dict_codbdi = pd.DataFrame(list(map_codbdi.items()), columns=['CODBDI', 'DESCRICAO_CODBDI'])\n",
    "    \n",
    "    map_tpmerc = {\n",
    "        '010': 'VISTA', '012': 'EXERCÍCIO DE OPÇÕES DE COMPRA', '013': 'EXERCÍCIO DE OPÇÕES DE VENDA',\n",
    "        '017': 'LEILÃO', '020': 'FRACIONÁRIO', '030': 'TERMO', '050': 'FUTURO COM RETENÇÃO DE GANHO',\n",
    "        '060': 'FUTURO COM MOVIMENTAÇÃO DIÁRIA', '070': 'OPÇÕES DE COMPRA', '080': 'OPÇÕES DE VENDA'\n",
    "    }\n",
    "    df_dict_tpmerc = pd.DataFrame(list(map_tpmerc.items()), columns=['TPMERC', 'DESCRICAO_TPMERC'])\n",
    "\n",
    "    path_codbdi = os.path.join(OUTPUTS_PATH, 'dicionario_codbdi.xlsx')\n",
    "    path_tpmerc = os.path.join(OUTPUTS_PATH, 'dicionario_tpmerc.xlsx')\n",
    "    \n",
    "    df_dict_codbdi.to_excel(path_codbdi, index=False)\n",
    "    df_dict_tpmerc.to_excel(path_tpmerc, index=False)\n",
    "    \n",
    "    print(f\" -> [SUCESSO] Dicionário de CODBDI salvo em: {path_codbdi}\")\n",
    "    print(f\" -> [SUCESSO] Dicionário de TPMERC salvo em: {path_tpmerc}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" -> [ERRO] Falha ao gerar os Dicionários de Códigos: {e}\")\n",
    "\n",
    "print(\"\\n--- GERAÇÃO DE ARQUIVOS AUXILIARES CONCLUÍDA ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74586d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SEGUNDO: Verificação e Análise\n",
    "# ==============================================================================\n",
    "print(\"\\n\\n--- INICIANDO VERIFICAÇÃO DE INTEGRIDADE DOS DADOS ---\")\n",
    "\n",
    "# Para a análise, podemos criar um DataFrame limpo sem modificar o original\n",
    "df_analise = df_final.copy()\n",
    "\n",
    "# Listas de colunas para verificação\n",
    "PRICE_COLS = ['PREABE', 'PREMAX', 'PREMIN', 'PREMED', 'PREULT', 'PREOFC', 'PREOFV', 'PREEXE', 'VOLTOT']\n",
    "INT_COLS = ['TIPREG', 'CODBDI', 'TPMERC', 'TOTNEG', 'QUATOT', 'FATCOT', 'INDOPC', 'DISMES', 'PRAZOT', 'PTOEXE']\n",
    "\n",
    "try:\n",
    "    # --- LIMPEZA PÓS-PROCESSAMENTO ---\n",
    "    initial_rows = len(df_analise)\n",
    "    df_analise.dropna(subset=['NOMRES'], inplace=True)\n",
    "    if len(df_analise) < initial_rows:\n",
    "        print(f\"Para fins de análise, foram removidos {initial_rows - len(df_analise):,} registros onde 'NOMRES' era nulo.\")\n",
    "\n",
    "    # --- 1. Verificação de Tipos de Dados ---\n",
    "    print(\"\\n1. Verificando os tipos de dados (Dtypes) das colunas:\")\n",
    "    display(df_analise.info(verbose=False))\n",
    "\n",
    "    # --- 2. Verificação de Valores Nulos ---\n",
    "    print(\"\\n2. Verificando a presença de valores nulos por coluna:\")\n",
    "    missing_values = df_analise.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "    if not missing_values.empty:\n",
    "        print(\"Colunas com valores nulos encontrados:\")\n",
    "        display(missing_values.to_frame('contagem_nulos'))\n",
    "    else:\n",
    "        print(\"[OK] Nenhuma coluna com valores nulos.\")\n",
    "        \n",
    "    # --- 4. Análise Descritiva das Colunas Numéricas ---\n",
    "    print(\"\\n4. Análise estatística descritiva das principais colunas numéricas:\")\n",
    "    display(df_analise[PRICE_COLS + ['TOTNEG', 'QUATOT']].describe().style.format('{:,.2f}'))\n",
    "    \n",
    "    print(\"\\n--- VERIFICAÇÃO DE INTEGRIDADE CONCLUÍDA ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Ocorreu um erro inesperado ao carregar ou verificar os dados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df632d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. Configuração dos Caminhos ---\n",
    "PROCESSED_PATH = os.path.join('data', 'processed')\n",
    "FINAL_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'dados_b3.parquet')\n",
    "ACOES_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'historico_acoes.parquet')\n",
    "\n",
    "print(\"--- INICIANDO FILTRAGEM PARA DATASET DE AÇÕES ---\")\n",
    "\n",
    "try:\n",
    "    df_completo = pd.read_parquet(FINAL_PARQUET_PATH)\n",
    "    print(f\"\\n[SUCESSO] Dataset completo carregado com {len(df_completo):,} registros.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO FATAL] Não foi possível carregar o dataset principal: {e}\")\n",
    "    # exit() # Descomente se estiver rodando como script .py\n",
    "\n",
    "# --- 2. Definição dos Filtros para Ações (COM TIPOS CORRIGIDOS) ---\n",
    "print(\"\\n2. Aplicando filtros para isolar apenas ações de empresas...\")\n",
    "\n",
    "# CORREÇÃO: Usar números inteiros (int) em vez de textos (str) para corresponder ao tipo de dado no DataFrame.\n",
    "filtro_codbdi = df_completo['CODBDI'].isin([2, 96])\n",
    "filtro_tpmerc = df_completo['TPMERC'].isin([10, 20])\n",
    "\n",
    "df_acoes = df_completo[filtro_codbdi & filtro_tpmerc].copy()\n",
    "\n",
    "# --- 3. Relatório da Filtragem ---\n",
    "registros_iniciais = len(df_completo)\n",
    "registros_finais = len(df_acoes)\n",
    "percentual_retido = (registros_finais / registros_iniciais) * 100 if registros_iniciais > 0 else 0\n",
    "\n",
    "print(f\" -> Registros iniciais: {registros_iniciais:,}\")\n",
    "print(f\" -> Registros após filtrar por ações: {registros_finais:,}\")\n",
    "print(f\" -> Percentual de dados retidos: {percentual_retido:.2f}%\")\n",
    "\n",
    "# --- 4. Seleção de Colunas Essenciais para ML ---\n",
    "colunas_essenciais = [\n",
    "    'DATA_PREGAO', 'CODISI', 'CODNEG', 'NOMRES', 'ESPECI',\n",
    "    'PREABE', 'PREMAX', 'PREMIN', 'PREULT', 'VOLTOT', 'QUATOT', 'FATCOT'\n",
    "]\n",
    "\n",
    "df_acoes = df_acoes[colunas_essenciais]\n",
    "print(f\"\\n3. Selecionadas {len(colunas_essenciais)} colunas essenciais para o dataset de Machine Learning.\")\n",
    "\n",
    "# --- 5. Salvando o Novo Dataset Filtrado ---\n",
    "try:\n",
    "    print(f\"\\n4. Salvando o dataset de ações em formato Parquet...\")\n",
    "    df_acoes.to_parquet(ACOES_PARQUET_PATH, index=False, compression='snappy')\n",
    "    print(f\" -> [SUCESSO] Dataset 'historico_acoes.parquet' salvo em: {ACOES_PARQUET_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\" -> [ERRO] Falha ao salvar o novo arquivo Parquet: {e}\")\n",
    "\n",
    "# --- 6. Verificação Final ---\n",
    "print(\"\\n--- Verificação Final do Dataset de Ações ---\")\n",
    "print(\"5 primeiras linhas do novo DataFrame 'df_acoes':\")\n",
    "display(df_acoes.tail())\n",
    "print(\"\\nInformações e tipos de dados do novo DataFrame:\")\n",
    "df_acoes.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
