{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import gc\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuração dos Diretórios ---\n",
    "RAW_PATH = os.path.join('data', 'raw')\n",
    "TEXTS_PATH = os.path.join('data', 'texts')\n",
    "PROCESSED_PATH = os.path.join('data', 'processed')\n",
    "\n",
    "# --- Caminho do Arquivo Final ---\n",
    "FINAL_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'dados_b3.parquet')\n",
    "\n",
    "# --- Criação dos Diretórios (se não existirem) ---\n",
    "os.makedirs(TEXTS_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "# --- Layout do Arquivo de Cotações Históricas ---\n",
    "COTAHIST_LAYOUT = {\n",
    "    'TIPREG': (1, 2), 'DATA_PREGAO': (3, 10), 'CODBDI': (11, 12), 'CODNEG': (13, 24),\n",
    "    'TPMERC': (25, 27), 'NOMRES': (28, 39), 'ESPECI': (40, 49), 'PRAZOT': (50, 52),\n",
    "    'MODREF': (53, 56), 'PREABE': (57, 69), 'PREMAX': (70, 82), 'PREMIN': (83, 95),\n",
    "    'PREMED': (96, 108), 'PREULT': (109, 121), 'PREOFC': (122, 134), 'PREOFV': (135, 147),\n",
    "    'TOTNEG': (148, 152), 'QUATOT': (153, 170), 'VOLTOT': (171, 188), 'PREEXE': (189, 201),\n",
    "    'INDOPC': (202, 202), 'DATVEN': (203, 210), 'FATCOT': (211, 217), 'PTOEXE': (218, 230),\n",
    "    'CODISI': (231, 242), 'DISMES': (243, 245),\n",
    "}\n",
    "\n",
    "COLSPECS = [(v[0] - 1, v[1]) for k, v in COTAHIST_LAYOUT.items()]\n",
    "NAMES = list(COTAHIST_LAYOUT.keys())\n",
    "\n",
    "# --- Definição das Colunas para Limpeza ---\n",
    "DATE_COLS = ['DATA_PREGAO', 'DATVEN']\n",
    "PRICE_COLS = ['PREABE', 'PREMAX', 'PREMIN', 'PREMED', 'PREULT', 'PREOFC', 'PREOFV', 'PREEXE', 'VOLTOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e203b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_arquivos_zip():\n",
    "    \"\"\"\n",
    "    Extrai arquivos .zip de data/raw para data/texts.\n",
    "    \"\"\"\n",
    "    zip_files = [f for f in os.listdir(RAW_PATH) if f.lower().endswith('.zip')]\n",
    "    if not zip_files:\n",
    "        print(\"Nenhum arquivo .zip encontrado em data/raw.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Encontrados {len(zip_files)} arquivos ZIP para extrair.\")\n",
    "    for filename in tqdm(zip_files, desc=\"Extraindo arquivos ZIP\"):\n",
    "        zip_path = os.path.join(RAW_PATH, filename)\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    base_name = os.path.basename(member)\n",
    "                    if not base_name.lower().endswith('.txt'):\n",
    "                        output_filename = os.path.join(TEXTS_PATH, base_name + '.TXT')\n",
    "                    else:\n",
    "                        output_filename = os.path.join(TEXTS_PATH, base_name)\n",
    "                    with zip_ref.open(member) as source, open(output_filename, 'wb') as target:\n",
    "                        shutil.copyfileobj(source, target)\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"AVISO: O arquivo '{filename}' não é um ZIP válido. Pulando.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERRO ao extrair '{filename}': {e}\")\n",
    "\n",
    "extrair_arquivos_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_e_consolidar_em_parquet_unico_final():\n",
    "    \"\"\"\n",
    "    Lê os arquivos de texto linha por linha para contornar a limitação do 'skipfooter',\n",
    "    processa em lotes para uso mínimo de memória, impõe um schema consistente e \n",
    "    anexa a um único arquivo Parquet.\n",
    "    \"\"\"\n",
    "    files_to_process = [f for f in os.listdir(TEXTS_PATH) if f.lower().endswith('.txt')]\n",
    "    if not files_to_process:\n",
    "        print(\"Nenhum arquivo .txt encontrado em data/texts para processar.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nIniciando consolidação de {len(files_to_process)} arquivos (Modo Linha-a-Linha).\")\n",
    "    \n",
    "    INT_COLS = ['TIPREG', 'CODBDI', 'TPMERC', 'TOTNEG', 'QUATOT', \n",
    "                'FATCOT', 'INDOPC', 'DISMES', 'PRAZOT', 'PTOEXE']\n",
    "    \n",
    "    BATCH_SIZE = 500_000 # Quantas linhas processar em memória por vez\n",
    "    writer = None\n",
    "    final_schema = None\n",
    "\n",
    "    if os.path.exists(FINAL_PARQUET_PATH):\n",
    "        os.remove(FINAL_PARQUET_PATH)\n",
    "        print(f\"Arquivo parquet antigo '{FINAL_PARQUET_PATH}' removido.\")\n",
    "\n",
    "    # Loop pelos NOMES DOS ARQUIVOS\n",
    "    for filename in tqdm(files_to_process, desc=\"Processando Arquivos\"):\n",
    "        file_path = os.path.join(TEXTS_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                # Lê todas as linhas para a memória (apenas para este arquivo)\n",
    "                # para podermos ignorar a primeira e a última\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # Ignora o header (primeira linha) e o trailer (última linha)\n",
    "            content_lines = lines[1:-1]\n",
    "            \n",
    "            # Se não houver conteúdo após remover header/trailer, pula para o próximo arquivo\n",
    "            if not content_lines:\n",
    "                continue\n",
    "\n",
    "            # Processa o conteúdo em lotes\n",
    "            for i in range(0, len(content_lines), BATCH_SIZE):\n",
    "                batch_lines = content_lines[i:i + BATCH_SIZE]\n",
    "                \n",
    "                # Converte o lote de linhas em um DataFrame\n",
    "                # Usamos um truque com StringIO para que o pandas leia a lista de strings\n",
    "                # como se fosse um arquivo.\n",
    "                from io import StringIO\n",
    "                data_io = StringIO(\"\".join(batch_lines))\n",
    "                \n",
    "                df_batch = pd.read_fwf(\n",
    "                    data_io,\n",
    "                    colspecs=COLSPECS,\n",
    "                    names=NAMES,\n",
    "                    header=None # Já removemos o header manualmente\n",
    "                )\n",
    "\n",
    "                if df_batch.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- LIMPEZA E IMPOSIÇÃO DE SCHEMA (aplicado ao lote) ---\n",
    "                for col in df_batch.select_dtypes(['object']).columns:\n",
    "                    df_batch[col] = df_batch[col].str.strip()\n",
    "                for col in DATE_COLS:\n",
    "                    df_batch[col] = pd.to_datetime(df_batch[col], format='%Y%m%d', errors='coerce')\n",
    "                for col in PRICE_COLS:\n",
    "                    df_batch[col] = pd.to_numeric(df_batch[col], errors='coerce') / 100\n",
    "                for col in INT_COLS:\n",
    "                    df_batch[col] = pd.to_numeric(df_batch[col], errors='coerce').astype('Int64')\n",
    "\n",
    "                # --- ESCRITA NO PARQUET ---\n",
    "                table = pa.Table.from_pandas(df_batch, preserve_index=False)\n",
    "                \n",
    "                if writer is None:\n",
    "                    final_schema = table.schema\n",
    "                    writer = pq.ParquetWriter(FINAL_PARQUET_PATH, final_schema, compression='snappy')\n",
    "                \n",
    "                table = table.cast(final_schema)\n",
    "                writer.write_table(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERRO CRÍTICO ao processar o arquivo '{filename}': {e}. Este arquivo será pulado.\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "        print(f\"\\n[SUCESSO] Todos os arquivos foram consolidados em:\\n{FINAL_PARQUET_PATH}\")\n",
    "    else:\n",
    "        print(\"\\nNenhum dado foi processado para o arquivo Parquet.\")\n",
    "\n",
    "processar_e_consolidar_em_parquet_unico_final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
